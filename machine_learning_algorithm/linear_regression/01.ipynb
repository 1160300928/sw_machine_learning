{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 线性回归再理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前已有的理解，使用假设函数$h(x)  = \\sum_{i=0}^{n}\\theta_ix_i = \\theta^Tx$来的得到我们预测的值。为了是预测值接近我们给的标签值y，使用目标(损失)函数$J(\\theta) = \\frac{1}{2}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2$,通过优化参数是的目标函数尽可能小，来得到我们的参数值$\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最小二乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过选择$\\theta$来最小化$J(\\theta)$。初始化$\\theta$, 然后使用梯度下降算法进行更新， $\\theta_j := \\theta_j - \\alpha\\frac{\\partial }{\\partial\\theta_j}J(\\theta)$。公式中计算偏导数的过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial }{\\partial\\theta_j}J(\\theta) = \\frac{\\partial }{\\partial\\theta_j}\\frac{1}{2}(h_\\theta(x) - y)^2\n",
    "\\\\= 2 \\cdot \\frac{1}{2}(h_\\theta(x) - y) \\cdot \\frac{\\partial }{\\partial\\theta_j}(h_\\theta(x) - y) \\\\= (h_\\theta(x) - y) \\cdot\n",
    "\\frac{\\partial }{\\partial\\theta_j}(\\sum_{i=0}^{n}\\theta_ix_i - y) \\\\= (h_\\theta(x) - y) \\cdot x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，可以得到梯度下降更新算法的公式  \n",
    "重复计算，直到收敛：$ \\theta_j := \\theta_j + \\alpha \\sum_{i=1}^{m}(y^{(i)} - h_\\theta(x^{(i)}))x_j^{(i)}$(对每一个j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准方程解法如下：$\\theta=(X^TX)^{-1}X^T\\bar{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可能性推断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是本文章重点介绍的内容，为什么使用最小均方作为损失度量，有其他的原因吗。下面给出一个可能性假设：  \n",
    "假设目标函数与目标值之间关系如下：  \n",
    "$y^{(i)} = \\theta^Tx^{(i)} + \\epsilon ^{(i)}$, 由目标函数加上一个误差项组成。  \n",
    "这里假设误差独立分布，满足高斯分布(标准分布)， 及 $ \\epsilon ^{(i)} \\sim \\mathcal N(0, \\sigma^2 )$, 均值为0， 方差为$\\sigma^2$的分布。\n",
    "可以得到其概率密度函数为：  \n",
    "$p(\\epsilon^{(i)}) = \\frac{1}{\\sqrt{2\\pi\\sigma}}exp\\left ( -\\frac{(\\epsilon^{(i)})^2}{2\\sigma^2}\\right )$, 可以转换成如下的形式：   \n",
    "$p(y^{{(i)}}| x^{(i)}; \\theta)) = \\frac{1}{\\sqrt{2\\pi\\sigma}}exp\\left ( -\\frac{(y^{(i)} - \\theta^T(x^{(i)})^2}{2\\sigma^2}\\right )$  \n",
    "表示在给定$x和\\theta情况下y$的分布。因为x和y是确定的，因此可以构造似然函数，求什么样的$\\theta$能满足上述条件。 \n",
    "似然函数如下：\n",
    "$L(\\theta) = L(\\theta:X, \\vec{y}) = p(\\vec y|X;\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于上面$\\epsilon^{(i)} $的独立性假设, 上述似然函数可以写作：  \n",
    "$L(\\theta) = \\prod_{i=1}^{m}p(\\vec y|X;\\theta) = \\prod_{i=1}^{m}\\frac{1}{\\sqrt{2\\pi\\sigma}}exp\\left ( -\\frac{(y^{(i)} - \\theta^T(x^{(i)})^2}{2\\sigma^2}\\right )$  \n",
    "因此，有如上的可能性模型，怎么去得到最好的参数。可以使用最大似然估计， 选择合适的$\\theta$ 来使似然函数去的最大值。因为$L(\\theta)$的单调性，可以取对数，其得到的极值点相同， 方便求导。其偏导数为0的点即为极值点。  \n",
    "$l(\\theta) = L(\\theta) \\\\ \n",
    "=log  \\prod_{i=1}^{m}p(\\vec y|X;\\theta) \\\\\n",
    "= log \\prod_{i=1}^{m}\\frac{1}{\\sqrt{2\\pi\\sigma}}exp\\left ( -\\frac{(y^{(i)} - \\theta^T(x^{(i)})^2}{2\\sigma^2}\\right ) \\\\\n",
    "=\\sum_{i=1}^{m} log \\frac{1}{\\sqrt{2\\pi\\sigma}}exp\\left ( -\\frac{(y^{(i)} - \\theta^T(x^{(i)})^2}{2\\sigma^2}\\right ) \\\\\n",
    "= m\\cdot log \\frac{1}{\\sqrt{2\\pi\\sigma}} - \\frac{1}{\\sigma^2} \\cdot \\frac{1}{2} \\sum_{i=1}^{m}(y^{(i)} - \\theta^Tx^{(i)})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过计算，要使函数去的最大值，则后一项尽可能的小，$\\frac{1}{2} \\sum_{i=1}^{m}(y^{(i)} - \\theta^Tx^{(i)})^2$。 可以解释开始的问题，为什么要用最小均方最为损失度量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归再理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实逻辑回归的大部分过程也和上面一样，包括独立性假设及其分布，最大似然的计算。下面简单叙述一下过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "与线性回归类似，通过给定的x预测y， 而y只有0，1两个取值，因此使用逻辑函数进行转换：  \n",
    "$h_\\theta(x) = g(\\theta^T x) = \\frac{1}{1+e^{-\\theta^Tx}},  g(z) = \\frac{1}{1+e^{-z}}$\n",
    "后续需要进行求导，求导如下：  \n",
    "${g}' = \\frac{d}{dz}\\frac{1}{1+e^{-z}} \\\\\n",
    "= \\frac{1}{(1+e^{-z})^2} (e^{-z}) \\\\\n",
    "=\\frac{1}{(1+e^{-z})^2}(1 - \\frac{1}{1 + e^{-z}}) \\\\\n",
    "= g(z)(1-g(z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了上述回归模型，怎么确定$\\theta$。类似上述线性回归模型，通过做一些可能性假设，求似然函数的最大值。\n",
    "假设以下情况： \n",
    "$P(y=1|x, \\theta) = h_\\theta(x)  \\\\ P(y=0|x, \\theta) =1 -  h_\\theta(x)$,   \n",
    "综合起来可以写成：  \n",
    "$P(y |x, \\theta) = (h_\\theta(x))^y(1 -  h_\\theta(x))^{1-y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设m训练实例独立生成，似然函数如下：  \n",
    "$L(\\theta) = p(\\vec{y}|X, \\theta) \\\\= \\prod_{i=1}^{m}p( y*{(i)}|x^{(i)}, \\theta) \\\\= \\prod_{i=1}^{m} (h_\\theta(x))^{y^{(i)}}(1 -  h_\\theta(x))^{1-y^{(i)}}$  \n",
    "去对数之后可以很轻易的求的:  \n",
    "$l(\\theta) = logL(\\theta) = p(\\vec{y}|X, \\theta) \\\\\n",
    "= \\sum_{i=1}^{m} {y^{(i)}}log(h(x^{(i)})) + (1-y^{(i)})log(1 -  h(x^{(i)}))$  \n",
    "下一步也是类似，怎么求得似然函数的最大值。使用梯度提升：更新参数的值：   \n",
    "$\\theta := \\theta + \\alpha \\bigtriangledown _\\theta l(\\theta)$  \n",
    "其梯度计算过程如下：  \n",
    "$\\frac{\\partial }{\\partial_j}l(\\theta) = \\left ( y\\frac{1}{g(\\theta^Tx)} - (1-y)\\frac{1}{1 - g(\\theta^Tx)} \\right )\\frac{\\partial }{\\partial_j}g(\\theta^Tx) \\\\= \\left ( y\\frac{1}{g(\\theta^Tx)} - (1-y)\\frac{1}{1 - g(\\theta^Tx)} \\right )g(\\theta^Tx)(1-g(\\theta^Tx))\\frac{\\partial }{\\partial_j}\\theta^Tx \\\\= (y(1-g(\\theta^Tx))  - (1-y)g(\\theta^Tx))x_j \\\\\n",
    "= (y-h_\\theta(x))x_j$   \n",
    "因此更新参数如下：  \n",
    "$ \\theta_j := \\theta_j + \\alpha \\sum_{i=1}^{m}(y^{(i)} - h_\\theta(x^{(i)}))x_j^{(i)}$(对每一个j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果比较上述两个参数更新的公式，可以发现是相同的，但注意，不是同一个算法，因为定义的$h(\\theta^Tx)$不是一样的。\n",
    "感知机的算法和推到过程也是类似的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
